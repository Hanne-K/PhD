---
title: "Other invertebrate datasets"
output: html_notebook
---

################################

## Loading required packages

################################


```{r load packages}
# Data management
library(here) # creating code with relative paths (all within the project folder)
library(rio) # convenience package for data import into R
library(tidyverse) # collection of packages that are loaded automatically: ggplot2, dplyr, tidyr,readr,purrr,tibble,stringr,forcats
library(knitr)
library(readxl) # read and write excel files
library(writexl)

# Graphics
library(sciplot) # bargraphs and lineplots
library(RColorBrewer) # color palettes, especially for nice looking maps

# Spatial files
library(sf) # spatial package
library(mapview) # interactive maps
library(geojsonsf) # converting between geojson and sf
library(rgdal) # for readOGR etc

```


################################

## Source files and import data

################################

## 1) NVE data

### NVE data: Magasin
```{r download NVE data: magasin }
## Data from NVE: Magasin
# UTM sone 33 (reccommended for the whole of Norway), geojson file, landsdekkende overlapp, magasiner. WGS84 (UTM) with lat long.
magasin_sf <- geojsonsf::geojson_sf(here::here("data","source_data","NVE_magasin","NVEData","Vannkraft_Magasin.geojson"))

# Change to projected coordinates
magasin_sf_P <- sf::st_transform(magasin_sf, 32633)

```

### NVE data: Innsjø
```{r download NVE data: innsjø }
## Data from NVE: innsjødatabase
innsjo_sf <- geojsonsf::geojson_sf(here::here("data","source_data","NVE_innsjo","NVEData","Innsjo_Innsjo.geojson"))

# Change to projected coordinates
innsjo_sf_P <- sf::st_transform(innsjo_sf, 32633)

```



### Data from GBIF: Multiple datasets

Initial download from GBIF via API. When using this dataset please use the following citation:
GBIF.org (14 December 2022) GBIF Occurrence Download https://doi.org/10.15468/dl.59hjre
DOI: https://doi.org/10.15468/dl.59hjre (may take some hours before being active)
Creation Date: 15:00:41 14 December 2022
Records included: 722752 records from 15 published datasets
Compressed data size: 150.3 MB
Download format: DWCA
```{r GBIF data download}
## Data from GBIF: The museum dataset for Norway
# GBIF.org (24 November 2022) GBIF Occurrence Download  https://doi.org/10.15468/dl.ee5z42
download_url <- "https://api.gbif.org/v1/occurrence/download/request/0216182-220831081235567.zip"
tmpfile <- tempfile()
tmpdir <- tempdir()
options(timeout = 500)
download.file(download_url,tmpfile)
occurrences_mDat <- rio::import(unzip(tmpfile,files="occurrence.txt",exdir = tmpdir), encoding = "UTF-8") # on the fly unzip and import to R object 

# Save locally
save(occurrences_mDat, file = here::here("data","source_data","occurrences_mDat.rda"))

```

Load occurrence dataset and create spatial object
```{r load GBIF file}
# Load saved file
load(file =  here::here("data","source_data","occurrences_mDat.rda"))

# Remove rows with missing coordinates
occurrences_mDat <- occurrences_mDat %>% drop_na(decimalLongitude)
  # from 722752 to 708289 observations

## Create a dataframe where each row is one location (based upon lat/long), summarized data
locations_mDat <- occurrences_mDat %>% 
  dplyr::group_by(locality, decimalLatitude,decimalLongitude) %>%
   dplyr::summarize(
    datasetName = paste0(unique(datasetName), collapse = ", "),
    publisher = paste0(unique(publisher), collapse = ", "),
    datasetID = paste0(unique(datasetID), collapse = ", "),
    datasetKey = paste0(unique(datasetKey), collapse = ", "),
    N_occurrences = length(unique(occurrenceID)),
    N_samplingEvents = length(unique(eventID)),
    locationIDs = paste(unique(locationID), collapse=", "),
    N_taxa = length(unique(scientificName)), # could change to taxonKey?
    N_methods = length(unique(samplingProtocol)),
    methods = paste0(unique(samplingProtocol), collapse = ", "),
    N_yrs = length(unique(year)),
    years = paste0(unique(year), collapse = ", "),
    first_year = min(year),
    last_year = max(year),
    period_yrs = (last_year - first_year),
    N_months = length(unique(month)),
    months = paste0(unique(month), collapse = ", "),
    field_number = paste0(unique(fieldNumber), collapse = ", "),
    collectionCode = paste0(unique(collectionCode), collapse = ", "),
    associatedReferences = paste0(unique(associatedReferences), collapse = ", ")
    ) 

## Tidy
# Which rows has missing longitude data?
# new_DF <- locations_mDat[is.na(locations_mDat$decimalLongitude),]
  # 1871 observations, will for now just remove these

# Remove rows with missing coordinates (done at occurrence level)
#locations_mDat <- locations_mDat %>% drop_na(decimalLongitude)
  # From 50076 observations to 48205, so the 1871 without long lat were removed

# Make dataframe
locations_mDat <- as.data.frame(locations_mDat) 

## Make location data-frame a spatial object 
loc_mDat_sf <- st_as_sf(locations_mDat, coords = c("decimalLongitude","decimalLatitude"), crs = 4326, remove = FALSE)
# Create a version with projected coordinates
loc_mDat_sf_P <- sf::st_transform(loc_mDat_sf, 32633)
# Check crs
st_crs(loc_mDat_sf)

```

################################

## Spatial filtering

################################

Want to attach info from the overlapping polygon to the point, allowing us to sum up info on number of points etc per hydropower magazine. 

Within magasine polygons
```{r within magasines}
## Sampling localities
#loc_mDat_sf_P

## Magasin polygons
#magasin_sf_P

## Add info from polygons to relevant points, and create subset
# Points within polygons
loc_mDat_mag_unfiltered <- st_join(loc_mDat_sf_P,magasin_sf_P, join = st_intersects, largest = TRUE) # largest = TRUE makes sure no extra rows with NAs are added to the dataframe
loc_mDat_mag <- loc_mDat_mag_unfiltered[magasin_sf_P, , op = st_intersects] # 1413 observations within polygons, with polygon info added.
```

Within 5 meter buffer (which includes the original polygons)
```{r 5 meter buffer}
## Points within polygons and 5m buffer
mag_buf_5m <- sf::st_buffer(magasin_sf_P,5) # 5 meter buffer

loc_mDat_mag_buf_5m_unfiltered <-  st_join(loc_mDat_sf_P,mag_buf_5m, join = st_intersects, largest = TRUE) # add info
loc_mDat_mag_buf_5m <- loc_mDat_mag_buf_5m_unfiltered[mag_buf_5m, , op = st_intersects] # filter, get 1555 observations

```

```{r only 5 meter buffer}
## Points only within the 5m buffer
# Creating two TRUE/FALSE vectors telling us if the corresponding loc element is in eac set of polygons.
buff <- lengths(st_intersects(loc_mDat_sf_P,mag_buf_5m)) > 0
magasins <- lengths(st_intersects(loc_mDat_sf_P,magasin_sf_P)) > 0 
# Filter, keep only localities within the buffer zone
within_mag_buf_5m <- loc_mDAt_mag_buf_5m_unfiltered %>% filter(buff & !magasins)

```


What localities do we find within 50 meters from magasines?
Present this data in a summary table.
```{r 50 meter buffer}

## Points within polygons and 50m buffer
mag_buf_50m <- sf::st_buffer(magasin_sf_P,50) # 50 meter buffer
loc_mDat_mag_buf_50m_unfiltered <-  st_join(loc_mDat_sf_P,mag_buf_50m, join = st_intersects, largest = TRUE) # add info
loc_mDat_mag_buf_50m <- loc_mDat_mag_buf_50m_unfiltered[mag_buf_50m, , op = st_intersects] # filter, get 2301 observations

```

### Do we have EPT data?

Found from these publisers: 
[1] "University of Oslo"                                  
[2] "The Norwegian Biodiversity Information Centre (NBIC)"
[3] "Norwegian Institute for Nature Research"             
[4] "Biofokus"                                            
[5] "University of Bergen"                                
[6] "UiT The Arctic University of Norway" 
```{r}
# Do we have any EPT observations? 
df_EPT <- occ_mDat_mag_50m %>% filter(order %in% c("Ephemeroptera","Plecoptera","Trichoptera"))
  # 5664 observations
# Publishers?
unique(df_EPT$publisher)
unique(df_EPT$preparations)
EPT_preparations <- c("ethanol 70%","Alcohol","ethanol 96%","Ethanol (80 %)")

df_EPT_ethanol <- df_EPT %>% filter(preparations %in% EPT_preparations)

# Where are these?
```


### Summary dataframe for all points within 50 meters from hydropower magasines.

Add magasine info to relevant occurrences
```{r}
# Relevant magasine info
magasin_liste <- c("decimalLatitude","decimalLongitude","elvenavnHierarki","vassdragsnummer","vannkraftverkNavn","vannkraftverkNr","delfeltNr","volumOppdemt_Mm3","magasinFormal_Liste","vatnLnr","magasinNr","hoyesteRegulerteVannstand_moh","lavesteRegulerteVannstand_moh","magasinNavn","magasinArealHRV_km2","status","idriftsattAar")

# Adding the info to all occurrences within the 50 meter buffer
occ_mDat_mag_50m <- dplyr::inner_join(occurrences_mDat,loc_mDat_mag_buf_50m[magasin_liste], by = c("decimalLatitude","decimalLongitude"))
```

Can we find relevant sampling methods?
```{r}
# Relevant sampling methods
unique(occ_mDat_mag_50m$samplingProtocol) # none of the specified methods are relevant...
df_trial <- occ_mDat_mag_50m %>% filter(samplingProtocol == "")
  # go from 18807 to 17918 occurrences, loose 899 occurrences - so will use this filter
  # Know that the specified ones are irrelevant, but the unspecified might also be, need to check later

```

### Make an excel file which summarises the info on a magasine level
```{r}
# Relevant types of magasines
unique(occ_mDat_mag_50m$magasinFormal_Liste)
magasintype_liste <- c("Kraftproduksjon","Kraftproduksjon, Rekreasjon, Vannforsyning","Kraftproduksjon, Vannforsyning","Kraftproduksjon, Akvakultur","Kraftproduksjon, Andre")


# Summary table for occurrences; sum up the info we have for each magasine polygon.
df_summary_mDat_mag_50m <- occ_mDat_mag_50m %>% 
  filter(status == "D") %>%
  filter(samplingProtocol == "") %>%
  filter(magasinFormal_Liste %in% magasintype_liste) %>%
  group_by(magasinNavn) %>%
  summarise(magasinNr = paste0(unique(magasinNr), collapse = ", "),
            idriftsattAar = paste0(unique(idriftsattAar), collapse = ", "),
            magasinArealHRV_km2 = paste0(unique(magasinArealHRV_km2), collapse = ", "),
            hoyesteRegulerteVannstand_moh = paste0(unique(hoyesteRegulerteVannstand_moh), collapse = ", "),
            lavesteRegulerteVannstand_moh = paste0(unique(lavesteRegulerteVannstand_moh), collapse = ", "),
            magasinFormal_Liste = paste0(unique(magasinFormal_Liste), collapse = ", "),
            vannkraftverkNavn = paste0(unique(vannkraftverkNavn), collapse = ", "),
            vassdragsnummer = paste0(unique(vassdragsnummer), collapse = ", "),
            elvenavnHierarki = paste0(unique(elvenavnHierarki), collapse = ", "),
            publisher = paste0(unique(publisher), collapse = ", "),
            insititutionCode = paste0(unique(institutionCode), collapse = ", "),
            collectionCode = paste0(unique(collectionCode), collapse = ", "),
            N_yrs = length(unique(year)),
            years = paste0(unique(year), collapse = ", "),
            first_year = min(year,na.rm = TRUE),
            last_year = max(year,na.rm = TRUE),
            LocatityNames = paste0(unique(locality), collapse = ", "),
            N_locations = length(unique(locationID)),
            orders = paste0(unique(order), collapse = ", ")
            )
  

library("writexl")
write_xlsx(df_summary_mag_50m,here::here("data","derived_data","df_summary_mag_50m.xlsx"))

write_xlsx(df_summary_mDat_mag_50m,here::here("data","derived_data","df_summary_mDat_mag_50m_19.12.22.xlsx"))

```

### Make a spatial file 

Summary of points within 50 meters which may be relevant.
```{r}
## Locality dataframe (a bit redundant, could just look at the loc_INH_mag - and double check coordinate crs)
# Summarizing

# Create a occurrence subset where unwanted data is filtered away, going from 18807 occrrences to 10251 occurrences
occ_mDat_mag_50m_filtered <- occ_mDat_mag_50m %>% 
  dplyr::filter(status == "D") %>%
  dplyr::filter(samplingProtocol == "") %>%
  dplyr::filter(magasinFormal_Liste %in% magasintype_liste) 
  
  
# Create a spatial file (when we include the filt above, number of localities drops from 2301 to 1534)
locations_mDat_50m  <- occ_mDat_mag_50m_filtered  %>% 
  dplyr::group_by(locality,decimalLatitude,decimalLongitude) %>%
  dplyr::summarize(
    publisher = paste0(unique(publisher), collapse = ", "),
    institutionCode = paste0(unique(institutionCode), collapse = ", "),
    collectionCode = paste0(unique(collectionCode), collapse = ", "),
    basisOfRecord = paste0(unique(basisOfRecord), collapse = ", "),
    catalogNumber = paste0(unique(catalogNumber), collapse = ", "),
    habitat = paste0(unique(habitat), collapse = ", "),
    N_samplingEvents = length(unique(eventID)),
    N_taxa = length(unique(scientificName)),
    orders = paste0(unique(order), collapse = ", "),
    N_individuals = sum(individualCount),
    N_yrs = length(unique(year)),
    years = paste0(unique(year), collapse = ", "),
    first_year = min(year, na.rm = TRUE),
    last_year = max(year, na.rm = TRUE),
    period_yrs = (last_year - first_year),
    N_months = length(unique(month)),
    months = paste0(unique(month), collapse = ", "),
    locationIDs = paste(unique(locationID), collapse=", "),
    field_number = paste0(unique(fieldNumber), collapse = ", "))

# Make dataframe
locations_mDat_50m <- as.data.frame(locations_mDat_50m) 

## Make location data-frame a spatial object 
locations_mDat_50m_sf <- st_as_sf(locations_mDat_50m, coords = c("decimalLongitude","decimalLatitude"), crs = 4326, remove = FALSE)
# Create a version with projected coordinates
locations_mDat_50m_sf_P <- sf::st_transform(locations_mDat_50m_sf, 32633)
    
```

Inspect
```{r}

mapview(locations_mDat_50m_sf_P, col.regions = "orange") + mapview(magasin_sf_P,col.regions = "blue", alpha.regions = 0.5)
```

### Dataset summary
What datasets contain the highest number of relevant datasets? Within 50m from hydropower magasines.
```{r}

df_datasets_mag_50m <- occ_mDat_mag_50m %>% 
  filter(status == "D") %>%
  filter(samplingProtocol == "") %>%
  filter(magasinFormal_Liste %in% magasintype_liste) %>%
  group_by(institutionCode) %>%
  summarise(N_occurrences = length(unique(occurrenceID)),
            N_yrs = length(unique(year)),
            years = paste0(unique(year), collapse = ", "),
            first_year = min(year, na.rm = TRUE),
            last_year = max(year, na.rm = TRUE),
            orders = paste0(unique(order), collapse = ", "),
            magasinNavn = paste0(unique(magasinNavn), collapse = ", "),
            publisher = paste0(unique(publisher), collapse = ", "),
            collectionCode = paste0(unique(collectionCode), collapse = ", "),
            )


  
write_xlsx(df_datasets_mag_50m,here::here("data","derived_data","df_datasets_mag_50m_19.12.22.xlsx"))

```

